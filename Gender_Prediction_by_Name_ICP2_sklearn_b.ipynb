{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinesh-umkc/kdm/blob/main/Gender_Prediction_by_Name_ICP2_sklearn_b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3vKn1HhWPGa"
      },
      "source": [
        "# Name Gender Identifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "R8y5VrHjWPGg",
        "outputId": "c7be3e21-3bec-4775-ef49-d51d29cfea54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'chane', 'first_letter': 'c', 'suffix1': 'e', 'suffix2': 'ne', 'suffix3': 'ane', 'gender': 'male'}, {'name': 'nydia', 'first_letter': 'n', 'suffix1': 'a', 'suffix2': 'ia', 'suffix3': 'dia', 'gender': 'female'}, {'name': 'jean-christophe', 'first_letter': 'j', 'suffix1': 'e', 'suffix2': 'he', 'suffix3': 'phe', 'gender': 'male'}, {'name': 'carolin', 'first_letter': 'c', 'suffix1': 'n', 'suffix2': 'in', 'suffix3': 'lin', 'gender': 'female'}, {'name': 'mace', 'first_letter': 'm', 'suffix1': 'e', 'suffix2': 'ce', 'suffix3': 'ace', 'gender': 'male'}]\n",
            "[{'name': 'chane', 'first_letter': 'c', 'suffix1': 'e', 'suffix2': 'ne', 'suffix3': 'ane'}, {'name': 'nydia', 'first_letter': 'n', 'suffix1': 'a', 'suffix2': 'ia', 'suffix3': 'dia'}, {'name': 'jean-christophe', 'first_letter': 'j', 'suffix1': 'e', 'suffix2': 'he', 'suffix3': 'phe'}, {'name': 'carolin', 'first_letter': 'c', 'suffix1': 'n', 'suffix2': 'in', 'suffix3': 'lin'}, {'name': 'mace', 'first_letter': 'm', 'suffix1': 'e', 'suffix2': 'ce', 'suffix3': 'ace'}]\n",
            "Gender Classification accuracy = 59.34550031466331\n"
          ]
        }
      ],
      "source": [
        "# Feature extractor\n",
        "def gender_features(word):\n",
        "    return {'last_letter': word[-1]}\n",
        "def gender_features2(name):\n",
        "    features = {}\n",
        "    features[\"first_letter\"] = name[0].lower()\n",
        "    features[\"last_letter\"] = name[-1].lower()\n",
        "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
        "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
        "    return features\n",
        "def gender_features3(word,gender):\n",
        "    features = {}\n",
        "    features[\"name\"] = word.lower()\n",
        "    features[\"first_letter\"] = word[0].lower()\n",
        "    features[\"suffix1\"] = word[-1].lower()\n",
        "    features[\"suffix2\"] = word[-2:].lower()\n",
        "    features[\"suffix3\"] = word[-3:].lower()\n",
        "    features[\"gender\"] = gender\n",
        "    return features\n",
        "\n",
        "def gender_features4(word):\n",
        "    features = {}\n",
        "    features[\"name\"] = word.lower()\n",
        "    features[\"first_letter\"] = word[0].lower()\n",
        "    features[\"suffix1\"] = word[-1].lower()\n",
        "    features[\"suffix2\"] = word[-2:].lower()\n",
        "    features[\"suffix3\"] = word[-3:].lower()\n",
        "    return features\n",
        "\n",
        "import nltk\n",
        "import random\n",
        "nltk.download('names')\n",
        "from nltk.corpus import names\n",
        "labeled_names = ([(name, 'female') for name in names.words('female.txt')] + [(name, 'male') for name in names.words('male.txt')])\n",
        "random.shuffle(labeled_names) # We shuffle the data so that we can split it by index into training and test data.\n",
        "labeled_names[:5]\n",
        "\n",
        "#Convert labeled names into feature sets v3\n",
        "featuresets3 = [gender_features3(n,gender) for (n, gender) in labeled_names]\n",
        "print(featuresets3[:5])\n",
        "import pandas as pd\n",
        "df_train = pd.json_normalize(featuresets3) \n",
        "df_train.head()\n",
        "featuresets4 = [gender_features4(n) for (n, gender) in labeled_names]\n",
        "print(featuresets4[:5])\n",
        "df_test = pd.json_normalize(featuresets4) \n",
        "#df_test.head()\n",
        "#print(df)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "train, valid = train_test_split(df_train, test_size=0.2)\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "train_set = count_vect.fit_transform(train['name'])\n",
        "train_tags = train['gender']\n",
        "\n",
        "valid_set = count_vect.transform(valid['name'])\n",
        "valid_tags = valid['gender']\n",
        "\n",
        "test_set = count_vect.transform(df_test['name'])\n",
        "\n",
        "#Naive Bayes classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB() \n",
        "\n",
        "clf.fit(train_set, train_tags) \n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "predictions_valid = clf.predict(valid_set)\n",
        "\n",
        "print('Gender Classification accuracy = {}'.format(\n",
        "        accuracy_score(predictions_valid, valid_tags) * 100)\n",
        "     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzgGQ61XWPGx"
      },
      "source": [
        "## More classifiers\n",
        "\n",
        "Scikit-learn (sklearn) is a popular library which features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN.\n",
        "\n",
        "NLTK provides an API to quickly use sklearn classifiers in `nltk.classify.scikitlearn`. The other option is to import and use sklearn directly.\n",
        "\n",
        "For an example of integrating sklearn with NLTK, you can check out [this](https://www.kaggle.com/alvations/basic-nlp-with-nltk) notebook on Kaggle. Kaggle is a great website for NLP and machine learning in general, creating an account is highly recommended.\n",
        "\n",
        "\n",
        "Scikit Learn:\n",
        "```python\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB() \n",
        "\n",
        "# To train the classifier, simple do \n",
        "clf.fit(train_set, train_tags) \n",
        "```\n",
        "\n",
        "NLTK: \n",
        "\n",
        "```\n",
        "ConditionalExponentialClassifier\n",
        "\n",
        "\n",
        "MaxentClassifier\n",
        "\n",
        "NaiveBayesClassifier\n",
        "\n",
        "WekaClassifier\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8RM8qt8WPGw"
      },
      "source": [
        "## Maximum entropy classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVP6y9AaWPGx"
      },
      "source": [
        "Scikit-learn (sklearn) is a popular library which features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN.\n",
        "\n",
        "NLTK provides an API to quickly use sklearn classifiers in `nltk.classify.scikitlearn`. The other option is to import and use sklearn directly.\n",
        "\n",
        "For an example of integrating sklearn with NLTK, you can check out [this](https://www.kaggle.com/alvations/basic-nlp-with-nltk) notebook on Kaggle. Kaggle is a great website for NLP and machine learning in general, creating an account is highly recommended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZWWZy6EWPGw"
      },
      "source": [
        "The principle of **maximum entropy** states that the probability distribution which best represents the current state of knowledge is the one with largest entropy.\n",
        "\n",
        "The principle of maximum entropy is invoked when we have some piece(s) of information about a probability distribution, but not enough to characterize it completely—likely because we do not have the means or resources to do so. As an example, if all we know about a distribution is its average, we can imagine infinite shapes that yield a particular average. The principle of maximum entropy says that we should humbly choose the distribution that maximizes the amount of unpredictability contained in the distribution.\n",
        "\n",
        "Taking the idea to the extreme, it wouldn’t be scientific to choose a distribution that simply yields the average value 100% of the time.\n",
        "\n",
        "From all the models that fit our training data, the Maximum Entropy classifier selects the one which has the largest entropy. Due to the minimum assumptions that the Maximum Entropy classifier makes, it is usually used when we don’t know anything about the prior distributions and when it is unsafe to make any assumptions. Also, the maximum entropy classifier is used when we can’t assume the conditional independence of the features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEwItUZXWPGw"
      },
      "outputs": [],
      "source": [
        "from nltk import MaxentClassifier\n",
        "\n",
        "me_classifier = MaxentClassifier.train(train_set3, max_iter=25) # max_iter has default value 100. In this example, the performance in terms of accuracy on the test set starts significantly improving beyond the previous model's at around 25 iterations."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H_G6W5wHqvuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My training improved when I used another classifier\n",
        "```python\n",
        " ==> Training (25 iterations)\n",
        "\n",
        "      Iteration    Log Likelihood    Accuracy\n",
        "      ---------------------------------------\n",
        "             1          -0.69315        0.370\n",
        "             2          -0.60155        0.630\n",
        "             3          -0.57970        0.630\n",
        "             4          -0.55965        0.637\n",
        "             5          -0.54133        0.666\n",
        "             6          -0.52462        0.701\n",
        "             7          -0.50938        0.730\n",
        "             8          -0.49549        0.753\n",
        "             9          -0.48283        0.765\n",
        "            10          -0.47126        0.778\n",
        "            11          -0.46068        0.787\n",
        "            12          -0.45098        0.792\n",
        "            13          -0.44208        0.798\n",
        "            14          -0.43389        0.800\n",
        "            15          -0.42633        0.802\n",
        "            16          -0.41935        0.805\n",
        "            17          -0.41289        0.806\n",
        "            18          -0.40689        0.808\n",
        "            19          -0.40132        0.809\n",
        "            20          -0.39612        0.811\n",
        "            21          -0.39128        0.812\n",
        "            22          -0.38674        0.812\n",
        "            23          -0.38250        0.812\n",
        "            24          -0.37851        0.813\n",
        "         Final          -0.37477        0.814\n",
        "        \n",
        "```\n",
        "\n",
        "Test Accuracy: \n",
        "```python\n",
        "round(accuracy(me_classifier, test_set3), 2) # Test accuracy\n",
        "0.8\n",
        "```"
      ],
      "metadata": {
        "id": "HwUTCfLfqwGM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hk7iGjdWPGx"
      },
      "outputs": [],
      "source": [
        "me_classifier.show_most_informative_features(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "-1.978 suffix2=='ia' and label is 'male'\n",
        "-1.921 suffix2=='na' and label is 'male'\n",
        "-1.515 suffix2=='sa' and label is 'male'\n",
        "-1.463 suffix1=='a' and label is 'male'\n",
        "-1.290 suffix2=='ra' and label is 'male'\n",
        "-1.278 suffix1=='k' and label is 'female'\n",
        "-1.197 suffix2=='rd' and label is 'female'\n",
        "-1.169 suffix2=='do' and label is 'female'\n",
        "-1.167 suffix2=='us' and label is 'female'\n",
        "-1.166 suffix2=='ta' and label is 'male'\n",
        "```"
      ],
      "metadata": {
        "id": "gBZhlcVZq_4I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0awLBiuzrBby"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}